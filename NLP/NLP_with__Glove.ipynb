{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_with _Glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_AXPzwyRHsN"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Activation, Dropout, Dense\r\n",
        "from keras.layers import Flatten, LSTM\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers.merge import Concatenate\r\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcR2OnlDROb_"
      },
      "source": [
        "def text_process(mess):\r\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\r\n",
        "\r\n",
        "    # Join the characters again to form the string.\r\n",
        "    nopunc = ''.join(nopunc)\r\n",
        "    \r\n",
        "    return nopunc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-dmTb0EROkH"
      },
      "source": [
        "X = df['Text_column'].apply(text_process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0klgIFQWROwN"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\r\n",
        "tokenizer.fit_on_texts(X)\r\n",
        "\r\n",
        "X_train = tokenizer.texts_to_sequences(X)\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "\r\n",
        "maxlen = 180 # Maximum_length of the sentence.\r\n",
        "\r\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LwUh7BiRO5y"
      },
      "source": [
        "from numpy import array\r\n",
        "from numpy import asarray\r\n",
        "from numpy import zeros\r\n",
        "\r\n",
        "embeddings_dictionary = dict()\r\n",
        "Glove_path='/./././'\r\n",
        "glove_file = open(Glove_path, encoding=\"utf8\")\r\n",
        "\r\n",
        "for line in glove_file:\r\n",
        "    records = line.split()\r\n",
        "    word = records[0]\r\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\r\n",
        "    embeddings_dictionary[word] = vector_dimensions\r\n",
        "glove_file.close()\r\n",
        "\r\n",
        "embedding_matrix = zeros((vocab_size, 300))\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    embedding_vector = embeddings_dictionary.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65QJuxi9RPGB"
      },
      "source": [
        "deep_inputs = Input(shape=(maxlen,))\r\n",
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(deep_inputs)\r\n",
        "LSTM_Layer_1 = LSTM(128)(embedding_layer)\r\n",
        "dense_layer_1 = Dense(25, activation='sigmoid')(LSTM_Layer_1)\r\n",
        "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYnKw9rYSf2n"
      },
      "source": [
        "history = model.fit(X_train, y, batch_size=128, epochs=25, verbose=1, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iExJgrP2SyXk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSHh7yiKSygs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}