{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R62-ZGcsJLko"
      },
      "source": [
        "# Recency given Dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVNsoEomIF1w"
      },
      "source": [
        "# RECENCY\r\n",
        "import datetime as dt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "today_date = dt.date(2016,3,30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1oSE3E2IF8p"
      },
      "source": [
        "df['Recency'] = df['Date_column'].apply(lambda x : (today_date - x).days)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "005DzOqFIGCq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY7N6f93Jqod"
      },
      "source": [
        "# Feature Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hPLqO51IGMn"
      },
      "source": [
        "def mean_encoder(var_name,target_name):\r\n",
        "  var_unique=list(X_train[var_name].unique())\r\n",
        "  var_dict={}\r\n",
        "  for item in var_unique:\r\n",
        "    value=X_train[X_train[var_name]==item][target_name].mean()\r\n",
        "    var_dict.update([(item,value)])\r\n",
        "  return var_dict\r\n",
        "\r\n",
        "def median_encoder(var_name,target_name):\r\n",
        "  var_unique=list(X_train[var_name].unique())\r\n",
        "  var_dict={}\r\n",
        "  for item in var_unique:\r\n",
        "    value=X_train[X_train[var_name]==item][target_name].median()\r\n",
        "    var_dict.update([(item,value)])\r\n",
        "  return var_dict\r\n",
        "\r\n",
        "def min_encoder(var_name,target_name):\r\n",
        "  var_unique=list(X_train[var_name].unique())\r\n",
        "  var_dict={}\r\n",
        "  for item in var_unique:\r\n",
        "    value=X_train[X_train[var_name]==item][target_name].min()\r\n",
        "    var_dict.update([(item,value)])\r\n",
        "  return var_dict\r\n",
        "\r\n",
        "def max_encoder(var_name,target_name):\r\n",
        "  var_unique=list(X_train[var_name].unique())\r\n",
        "  var_dict={}\r\n",
        "  for item in var_unique:\r\n",
        "    value=X_train[X_train[var_name]==item][target_name].max()\r\n",
        "    var_dict.update([(item,value)])\r\n",
        "  return var_dict\r\n",
        "\r\n",
        "def sum_encoder(var_name,target_name):\r\n",
        "  var_unique=list(X_train[var_name].unique())\r\n",
        "  var_dict={}\r\n",
        "  for item in var_unique:\r\n",
        "    value=X_train[X_train[var_name]==item][target_name].sum()\r\n",
        "    var_dict.update([(item,value)])\r\n",
        "  return var_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBT41X1KJzUw"
      },
      "source": [
        "category_feats=['.....']\r\n",
        "continuos_feats=['....']\r\n",
        "for tar in continuos_feats:\r\n",
        "  for var in category_feats:\r\n",
        "    X_train[var+'_mean']=X_train[var].map(mean_encoder(var,tar))\r\n",
        "    X_test[var+'_mean']=X_test[var].map(mean_encoder(var,tar))\r\n",
        "    X_train[var+'_median']=X_train[var].map(median_encoder(var,tar))\r\n",
        "    X_test[var+'_median']=X_test[var].map(median_encoder(var,tar))\r\n",
        "    X_train[var+'_min']=X_train[var].map(min_encoder(var,tar))\r\n",
        "    X_test[var+'_min']=X_test[var].map(min_encoder(var,tar))\r\n",
        "    X_train[var+'_max']=X_train[var].map(max_encoder(var,tar))\r\n",
        "    X_test[var+'_max']=X_test[var].map(max_encoder(var,tar))\r\n",
        "    X_train[var+'_sum']=X_train[var].map(sum_encoder(var,tar))\r\n",
        "    X_test[var+'_sum']=X_test[var].map(sum_encoder(var,tar))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovEMf5krJzai"
      },
      "source": [
        "for col in X_train.columns:\r\n",
        "    try:\r\n",
        "        X_train[col].replace([np.inf,-np.inf],np.nan,inplace=True)\r\n",
        "        X_train[col]=X_train[col].fillna(np.mean(X_train[col]))\r\n",
        "    except:\r\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HiwOxizJzgX"
      },
      "source": [
        "for cols in X_test.columns:\r\n",
        "    try:\r\n",
        "        X_test[cols].replace([np.inf,-np.inf],np.nan,inplace=True)\r\n",
        "        X_test[cols]=X_test[cols].fillna(np.mean(X_test[cols]))\r\n",
        "    except:\r\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DchADuXyJzoA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-G02xBHIGT_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}